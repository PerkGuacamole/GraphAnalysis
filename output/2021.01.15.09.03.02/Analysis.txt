Graphs:
	Need larger graphs for the next run. 1GB+ in size.

BFS:
	If a graph is disconnected, an iteration might complete instantly.
	Maybe if it runs too fast, we choose another vertex instead for that iteration.
	Actually, I don't like that idea, maybe that's just something to note for certain graphs and test iterations.

Memory Benchmark - Latency:
	Fix the output, its reporting in ns instead of s for time elapsed. - DONE
	176,289,939,677 DRAM Read Random
	 63,789,283,634 PMEM Read Random

	These tests also took too long.
	Maybe cap the number of latency loads, 1GB should be good. -  DONE

Memory Benchmark - Read Linear:
	559,326,034,501 DRAM
	557,043,403,011 PMEM
	These seem too high.

	222,973,153,506
	179,209,107,078

Get Memory speed:
	sudo dmidecode --type 17
	Need to calculate the theoretical maximum
	For my machine:
	    3.6G * 2 * 8 * 2 = 115.2GB/s
